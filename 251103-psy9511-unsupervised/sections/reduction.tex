\section{Dimensionality reduction}


\newcommand{\motivationplot}[1]{
    \begin{tikzpicture}
        \begin{axis}[
            height=7cm,
            width=7cm,
            xlabel=$x_1$,
            ylabel=$x_2$,
            xmajorticks=false,
            ymajorticks=false
        ]
            \ifnum#1=0
                \addplot[
                    only marks,
                    blue,
                    opacity=0.5
                ] coordinates {
                    (-0.025, -0.056) %0
                    (-0.017, 0.026) %1
                    (-0.061, -0.061) %2
                    (0.01, -0.005) %3
                    (-0.073, -0.022) %4
                    (0.126, 0.126) %5
                    (0.132, 0.1) %6
                    (0.063, 0.121) %7
                    (0.099, 0.165) %8
                    (0.104, 0.079) %9
                    (0.16, -0.11) %10
                    (0.138, -0.157) %11
                    (0.153, -0.081) %12
                    (0.11, -0.1) %13
                    (0.122, -0.114) %14
                };
            \fi
            \ifnum#1=1
                \addplot[
                    only marks,
                    blue,
                    opacity=0.5
                ] (x, x + rand);
            \fi
        \end{axis}
    \end{tikzpicture}
}

\newsavebox{\motivationclusters}
\sbox{\motivationclusters}{
    \motivationplot{0}
}
\newsavebox{\motivationaxes}
\sbox{\motivationaxes}{
    \motivationplot{1}
}

\newsavebox{\overfitting}
\sbox{\overfitting}{
    \begin{tikzpicture}
        \begin{axis}[
            height=6cm,
            width=8cm,
            axis lines=left,
            xmajorticks=false,
            ymajorticks=false,
            ylabel=Loss,
            xlabel=$|p|$,
            ymin=-2,
            xmin=-1,
            xmax=3
        ]
            \addplot[thick, red, domain=-1:3, samples=100] (x, x^2);
        \end{axis}
    \end{tikzpicture}
}

\newcommand{\predictorplot}[3]{
    \nextgroupplot[
        xlabel=\footnotesize{#1},
        ylabel=\footnotesize{#2},
        ytick=#3
    ]
        \addplot[
            only marks,
            blue,
            opacity=0.5
        ] table [
            col sep=comma,
            x=#1,
            y=mpg
        ] {data/scaledauto.csv};
}

\newcommand{\coefplot}[4]{
    \predictorplot{#1}{#2}{#3}
        \addplot[
            red
        ] coordinates {
            (-5, 23.445-5*#4)
            (5, 23.445+5*#4)
        };
}

\newcommand{\correlationplot}[1]{
    \begin{tikzpicture}
        \begin{groupplot}[
            group style={
                group size=3 by 2,
                vertical sep=0.8cm,
                horizontal sep=0.05cm
            },
            height=4cm,
            width=4.6cm,
            ticklabel style={font=\footnotesize},
            xtick pos=bottom,
            ytick pos=left,
            x label style={yshift=0.15cm},
            y label style={yshift=-0.15cm},
            ymin=0,
            ymax=52,
            xmin=-4,
            xmax=4,
            xtick={-3, 0, 3}
        ]
            \ifnum#1=0
                \predictorplot{cylinders}{mpg}{{10, 20, 30, 40, 50}}
                \predictorplot{displacement}{}{\empty}
                \predictorplot{horsepower}{}{\empty}
                \predictorplot{weight}{mpg}{{10, 20, 30, 40, 50}}
                \predictorplot{acceleration}{}{\empty}
                \predictorplot{year}{}{\empty}
            \fi
            \ifnum#1=1
                \coefplot{cylinders}{mpg}{{10, 20, 30, 40, 50}}{-0.561}
                \coefplot{displacement}{}{\empty}{0.802}
                \coefplot{horsepower}{}{\empty}{-0.015}
                \coefplot{weight}{mpg}{{10, 20, 30, 40, 50}}{-5.763}
                \coefplot{acceleration}{}{\empty}{0.234}
                \coefplot{year}{}{\empty}{2.771}
            \fi
        \end{groupplot}
    \end{tikzpicture}
}

\newsavebox{\correlationpredictors}
\sbox{\correlationpredictors}{
    \correlationplot{0}
}
\newsavebox{\correlationcoefs}
\sbox{\correlationcoefs}{
    \correlationplot{1}
}

\newsavebox{\heatmap}
\sbox{\heatmap}{
    \begin{tikzpicture}
        \foreach \y [count=\n] in {
            {1,0.30,0.86,0.89,0.41,0.93},
            {0.30,1,0.41,0.34,0.29,0.36},
            {0.86,0.41,1,0.84,0.68,0.89},
            {0.89,0.34,0.84,1,0.50,0.95},
            {0.41,0.29,0.68,0.50,1,0.54},
            {0.93,0.36,0.89,0.95,0.54,1},
          } {
            \foreach \x [count=\m] in \y {
                \pgfmathsetmacro{\colorfraction}{100*\x} % Multiply by 100
              \node[fill=red!\colorfraction!yellow, minimum size=1cm, text=white, inner sep=0pt, outer sep=0pt] at (\m,-\n) {\x};
            }
          }

      \end{tikzpicture}
}

\begin{frame}{Dimensionality reduction: Motivation}
    \begin{tikzpicture}
        \node[] at (-5.25, 3.5) {};
        \node[] at (-5.25, -3.5) {};

        \visible<1>{
            \node[] at (0, 0) {
                \usebox{\motivationclusters}
            };
        }
        \visible<2>{
            \node[] at (0, 0) {
                \usebox{\motivationaxes}
            };
        }
        \visible<3>{
            \node[draw=black, inner sep=0pt, anchor=west] (test) at (-4, 0) {
                \includegraphics[height=4cm]{data/iqtest.png}
            };
            \node[draw=black, dashed, anchor=east, align=center] (iq) at (4, 0) {
                Intelligence\\quotient
            };
            \draw[-stealth, line width=5pt, gray] (test) -- (iq);
        }
        \visible<4>{
            \node[] at (0, 0) {
                \usebox{\overfitting}
            };
        }
        \visible<5>{
            \node[] at (0, 0) {
                \usebox{\correlationpredictors}
            };
        }
        \visible<6>{
            \node[] at (0, 0) {
                \usebox{\correlationcoefs}
            };
        }
        \visible<7-8>{
            \node[] (heatmap) at (0, 0) {
                \usebox{\heatmap}
            };
        }
        \visible<8>{
            \node[minimum size=1cm, draw=black, line width=2pt] at ($ (heatmap.north) + (0.5, -2.62) $) {};
        }
    \end{tikzpicture}
\end{frame}


\newcommand{\pcaplot}[1]{
    \begin{tikzpicture}
        \begin{axis}[
            height=6.5cm,
            width=6.5cm,
            xlabel=$z_\mathrm{weight}$,
            ylabel=$z_\mathrm{horsepower}$,
            xmin=-2,
            xmax=3.5,
            ymin=-2,
            ymax=3
        ]
            \addplot[
                only marks,
                mark=*,
                color=blue,
                opacity=0.1
            ] table [col sep=comma, x=x, y=y] {data/pca_components.csv};

            \ifnum#1>0
                \ifnum#1<8
                    \addplot[only marks, mark=*, color=red] coordinates {
                        (-1.81260902e-16, -1.81260902e-17)
                    };
                \fi
            \fi

            \ifnum#1<6
                \ifnum#1=2
                    \draw[red,-stealth,very thick] (axis cs: -1.81260902e-16, -1.81260902e-17) -- (axis cs: -1.81260902e-16 + 0.96677463, -1.81260902e-17 + 0.96677463);
                    \node[anchor=north, outer sep=3pt] at (axis cs: -1.81260902e-16 + 0.5 * 0.96677463, -1.81260902e-17 + 0.5 * 0.96677463) {\Large{\textbf{\textcolor{red}{$v$}}}};
                \fi

                \ifnum#1>2
                    \draw[red,-stealth,very thick] (axis cs: -1.81260902e-16 - 5 * 0.96677463, -1.81260902e-17 - 5 * 0.96677463) -- (axis cs: -1.81260902e-16 + 5 * 0.96677463, -1.81260902e-17 + 5 * 0.96677463);
                    \draw[red,|-|,very thick] (axis cs: -1.81260902e-16 - 1 * 0.96677463, -1.81260902e-17 - 1 * 0.96677463) -- (axis cs: -1.81260902e-16 + 1 * 0.96677463, -1.81260902e-17 + 1 * 0.96677463);
                    \draw[red,|-|,very thick] (axis cs: -1.81260902e-16 - 2 * 0.96677463, -1.81260902e-17 - 2 * 0.96677463) -- (axis cs: -1.81260902e-16 + 2 * 0.96677463, -1.81260902e-17 + 2 * 0.96677463);

                    \node[rotate=45, anchor=north, inner sep=5pt] at (axis cs: -1.81260902e-16 - 1 * 0.96677463, -1.81260902e-17 - 1 * 0.96677463) {\Large{\textbf{\textcolor{red}{-1}}}};
                    \node[rotate=45, anchor=north, inner sep=5pt] at (axis cs: -1.81260902e-16 - 2 * 0.96677463, -1.81260902e-17 - 2 * 0.96677463) {\Large{\textbf{\textcolor{red}{-2}}}};
                    \node[rotate=45, anchor=north, inner sep=5pt] at (axis cs: -1.81260902e-16 + 1 * 0.96677463, -1.81260902e-17 + 1 * 0.96677463) {\Large{\textbf{\textcolor{red}{1}}}};
                    \node[rotate=45, anchor=north, inner sep=5pt] at (axis cs: -1.81260902e-16 + 2 * 0.96677463, -1.81260902e-17 + 2 * 0.96677463) {\Large{\textbf{\textcolor{red}{2}}}};
                    \node[rotate=45, anchor=south, inner sep=3pt] at (axis cs: -1.81260902e-16 + 2.5 * 0.96677463, -1.81260902e-17 + 2.5 * 0.96677463) {\Large{\textbf{\textcolor{red}{$PC_1$}}}};
                \fi

                \ifnum#1=4
                    \draw[red,-stealth,very thick] (axis cs: -1.81260902e-16, -1.81260902e-17) -- (axis cs: -1.81260902e-16 + 3 * 0.26058464, -1.81260902e-17 - 3 * 0.26058464);
                    \node[anchor=west, outer sep=3pt] at (axis cs: -1.81260902e-16 + 0.8 * 0.26058464, -1.81260902e-17 - 0.8 * 0.26058464) {\Large{\textbf{\textcolor{red}{$v$}}}};
                \fi
                \ifnum#1>4
                    \draw[red,|-|,very thick] (axis cs: -1.81260902e-16 - 3 * 0.26058464, -1.81260902e-17 + 3 * 0.26058464) -- (axis cs: -1.81260902e-16 + 3 * 0.26058464, -1.81260902e-17 - 3 * 0.26058464);
                    \draw[red,|-|,very thick] (axis cs: -1.81260902e-16 - 6 * 0.26058464, -1.81260902e-17 + 6 * 0.26058464) -- (axis cs: -1.81260902e-16 + 6 * 0.26058464, -1.81260902e-17 - 6 * 0.26058464);
                    \draw[red,|-|,very thick] (axis cs: -1.81260902e-16 - 9 * 0.26058464, -1.81260902e-17 + 9 * 0.26058464) -- (axis cs: -1.81260902e-16 + 9 * 0.26058464, -1.81260902e-17 - 9 * 0.26058464);
                \fi
            \fi

            \ifnum#1=6
                \draw[red,-stealth,very thick] (axis cs: -1.81260902e-16, -1.81260902e-17) -- (axis cs: -1.81260902e-16 + -0.1, -1.81260902e-17 + -0.8);
                \node[anchor=west, outer sep=3pt] at (axis cs: -1.81260902e-16 + 0.5 * -0.1, -1.81260902e-17 + 0.5 * -0.8) {\Large{\textbf{\textcolor{red}{$w$}}}};
            \fi
            \ifnum#1=7
                \draw[red,-stealth,very thick] (axis cs: -1.81260902e-16, -1.81260902e-17) -- (axis cs: -1.81260902e-16 + -0.1, -1.81260902e-17 + 0.8);
                \node[anchor=west, outer sep=3pt] at (axis cs: -1.81260902e-16 + 0.5 * -0.1, -1.81260902e-17 + 0.5 * 0.8) {\Large{\textbf{\textcolor{red}{$u$}}}};
            \fi
            \ifnum#1=8
                \draw[red,-stealth,very thick] (axis cs: -1.81260902e-16 + 1, -1.81260902e-17 - 8) -- (axis cs: -1.81260902e-16 + -1, -1.81260902e-17 + 8);
                \node[anchor=south, outer sep=3pt, rotate=280] at (axis cs: 0.05, -1.7) {\Large{\textbf{\textcolor{red}{$Z_0$}}}};
            \fi
        \end{axis}
    \end{tikzpicture}
}

\newsavebox{\pcavariables}
\sbox{\pcavariables}{
    \pcaplot{0}
}
\newsavebox{\pcacenter}
\sbox{\pcacenter}{
    \pcaplot{1}
}
\newsavebox{\pcavector}
\sbox{\pcavector}{
    \pcaplot{2}
}
\newsavebox{\pcafirst}
\sbox{\pcafirst}{
    \pcaplot{3}
}
\newsavebox{\pcasecondvector}
\sbox{\pcasecondvector}{
    \pcaplot{4}
}
\newsavebox{\pcasecond}
\sbox{\pcasecond}{
    \pcaplot{5}
}
\newsavebox{\pcaica}
\sbox{\pcaica}{
    \pcaplot{6}
}
\newsavebox{\pcapls}
\sbox{\pcapls}{
    \pcaplot{7}
}
\newsavebox{\pcaz}
\sbox{\pcaz}{
    \pcaplot{8}
}

\newsavebox{\pcadata}
\sbox{\pcadata}{
    \begin{tikzpicture}
        \begin{axis}[
            height=6.5cm,
            width=6.5cm,
            xlabel=$PC_1$,
            ylabel=$PC_2$,
        ]
            \addplot[
                only marks,
                mark=*,
                color=blue,
                opacity=0.25
            ] table [col sep=comma, x=x, y=y] {data/pcs.csv};
        \end{axis}
    \end{tikzpicture}
}

\newcommand{\screeplot}[1]{
    \begin{tikzpicture}
        \begin{axis}[
            height=5.5cm,
            width=5.5cm,
            xlabel=\footnotesize{Component},
            ylabel=\footnotesize{Explained variance},
            xtick pos=bottom,
            ytick pos=left,
            ticklabel style={font=\footnotesize},
            xmin=-0.5,
            xmax=5.5
        ]
            \addplot[
                mark=*,
                blue
            ] coordinates {
                (0, 0.709)
                (1, 0.139)
                (2, 0.112)
                (3, 0.022)
                (4, 0.010)
                (5, 0.006)
            };

            \ifnum#1=1
                \draw[thick, red] (axis cs: -0.5, 0.2) -- (axis cs: 5.5, 0.2);
            \fi

            \ifnum#1=2
                \draw[thick, red] (axis cs: -0.5, 0.06) -- (axis cs: 5.5, 0.06);
            \fi
        \end{axis}
    \end{tikzpicture}
}

\newsavebox{\screedata}
\sbox{\screedata}{
    \screeplot{0}
}
\newsavebox{\screefirst}
\sbox{\screefirst}{
    \screeplot{1}
}
\newsavebox{\screesecond}
\sbox{\screesecond}{
    \screeplot{2}
}

\begin{frame}{Dimensionality reduction: Principal component analysis}
    \begin{tikzpicture}
        \node[] at (-5.25, 3.5) {};
        \node[] at (-5.25, -3.5) {};

        \visible<1>{
            \node[] at (0, 0.5) {
                \usebox{\pcavariables}
            };
        }
        \visible<2>{
            \node[] at (0, 0.5) {
                \usebox{\pcacenter}
            };
            \node[] at (0, -3) {
                $c \rightarrow$ center of the data
            };
        }
        \visible<3>{
            \node[] at (0, 0.5) {
                \usebox{\pcavector}
            };
            \node[] at (0, -3) {
                $v \rightarrow$ direction of maximum variance
            };
        }
        \visible<4>{
            \node[] at (0, 0.5) {
                \usebox{\pcafirst}
            };
            \node[] at (0, -3) {
                $PC_1 \rightarrow 0.69*z_\mathrm{horsepower}+0.71*z_\mathrm{weight}$
            };
        }
        \visible<5>{
            \node[] at (0, 0.5) {
                \usebox{\pcasecondvector}
            };
            \node[] at (0, -3) {
                $v \rightarrow$ direction of maximum variance \textbf{orthogonal} to $PC_1$
            };
        }
        \visible<6>{
            \node[] at (0, 0.5) {
                \usebox{\pcasecond}
            };
        }
        \visible<7>{
            \node[] at (0, 0.5) {
                \usebox{\pcadata}
            };
        }
        \visible<8-9>{
            \node[] at (0, 0.5) {
                \begin{tabular}{|c|c|c|c|c|}
                    \hline
                    \textbf{mpg}&\textbf{horsepower}&\textbf{weight}&\textbf{PC1}&\textbf{PC2}\\
                    \hline
                    18&130&3504&0.908&0.303\\
                    \hline
                    15&165&3693&1.709&0.517\\
                    \hline
                    18&150&3436&1.219&0.455\\
                    \hline
                    16&150&3433&1.217&0.457\\
                    \hline
                    17&140&3449&1.046&0.260\\
                    \hline
                    15&198&4341&2.856&0.583\\
                    \hline
                    14&220&4354&3.272&0.977\\
                    \hline
                \end{tabular}
            };
        }
        \visible<9>{
            \node[] at (0, -2.5) {
                $PC_1 = 0.69*z_\mathrm{horsepower}+0.71*z_\mathrm{weight}$
            };
        }
        \visible<10>{
            \node[font=\small\selectfont] at (0, 0) {
                \url{http://localhost:8888/notebooks/notebooks\%2FPCA.ipynb}
            };
        }
        \visible<11-12,15>{
            \node[] at (0, 1) {
                \usebox{\screedata}
            };
        }
        \visible<12,15>{
            \node[font=\small\selectfont] at (0, -2.5) {
                $\hat{y}=\beta_0+\sum_{i=0}^n\beta_i PC_i$
            };
        }
        \visible<13>{
            \node[] at (0, 1) {
                \usebox{\screefirst}
            };
            \node[font=\small\selectfont] at (0, -2.5) {
                $\hat{y}=\beta_0+\sum_{i=1}^{{\color{red}1}}\beta_i PC_i$
            };
        }
        \visible<14>{
            \node[] at (0, 1) {
                \usebox{\screesecond}
            };
            \node[font=\small\selectfont] at (0, -2.5) {
                $\hat{y}=\beta_0+\sum_{i=1}^{{\color{red}3}}\beta_i PC_i$
            };
        }
        \visible<15>{
            \node[font=\small\selectfont, text=red] at (0, -3.2) {
                $n$ decided via a validation set, tested in a \textbf{held-out test set}
            };
        }
        \visible<16-17>{
            \node[align=left, text width=10.5cm] at (0, 0) {
                \underline{Principal component analysis}: Transforms our dataset by computing \textit{principal components} to replace our original variables.
                \begin{itemize}
                    \item Principal components are:
                    \begin{itemize}
                        \item Linear combinations of the original variables
                        \item \alert<17>{Orthogonal to each other, meaning that they capture different signals in our data (linearly uncorrelated)}
                    \end{itemize}
                    \item They can be useful for:
                    \begin{itemize}
                        \item (Qualitatively) understanding the signal in our data
                        \item Reducing the number of predictors for modelling
                    \end{itemize}
                \end{itemize}
            };
        }
    \end{tikzpicture}
\end{frame}

\begin{frame}{Dimensionality reduction: Independent component analysis}
    \begin{tikzpicture}
        \node[] at (-5.25, 3.5) {};
        \node[] at (-5.25, -3.5) {};

        \visible<1>{
            \node[inner sep=0pt, draw=black] at (0, 0) {
                \includegraphics[width=5cm]{data/ica_illustration.jpg}
            };
        }
        \visible<2>{
            \node[text width=10.5cm] (pca) at (0, 2) {
                \underline{Principal component analysis}: Create orthogonal components that are linear combinations of our variables:\\
                $PC_0=\beta_0x_0+\beta_0x_0+\ldots+\beta_nx_n$,\\
                $PC_1=\gamma_0x_0+\gamma_1x_1+\ldots+\gamma_nx_n$,\\
                $PC_0 \perp PC_1$ (e.g. no linear correlation)
            };
            \node[text width=10.5cm] (ica) at (0, -1) {
                \underline{Independent component analysis}: Represent each of our original variables as a linear combination of underlying sources:\\
                $x_0=\alpha_{00}s_0+\alpha_{01}s_1+\ldots+\alpha_{0n}s_n$,\\
                $x_1=\alpha_{10}s_0+\alpha_{11}s_1+\ldots+\alpha_{1n}s_n$,\\
                $s_0 \perp\!\!\!\perp s_1$ (e.g. statistically independent)
            };
        }
        \visible<3>{
            \node[] at (0, 0.5) {
                \usebox{\pcavector}
            };
            \node[] at (0, -3) {
                $v \rightarrow$ direction of greatest variance
            };
        }
        \visible<4>{
            \node[] at (0, 0.5) {
                \usebox{\pcaica}
            };
            \node[] at (0, -3) {
                $w \rightarrow$ direction that maximizes the non-Gaussanity of $w^TX$
            };
        }
        \visible<5>{
            \node[text width=10.5cm, align=center, font=\tiny\selectfont] at (0, 0) {
                \url{https://scikit-learn.org/dev/auto_examples/decomposition/plot_ica_blind_source_separation.html\#sphx-glr-auto-examples-decomposition-plot-ica-blind-source-separation-py}
            };
        }
        \visible<6>{
            \node[text width=10.5cm, align=center, font=\footnotesize\selectfont] at (0, 0) {
                \url{https://pmc.ncbi.nlm.nih.gov/articles/PMC7162660/}
            };
        };

    \end{tikzpicture}
\end{frame}

\newcommand{\plsplot}[1]{
    \begin{tikzpicture}
        \begin{axis}[
            height=6.5cm,
            width=6.5cm,
            xlabel=$z_\mathrm{weight}*$,
            ylabel=$z_\mathrm{horsepower}*$,
            xmin=-2,
            xmax=3.5,
            ymin=-2,
            ymax=3
        ]
            \addplot[
                only marks,
                mark=*,
                color=blue,
                opacity=0.1
            ] table [col sep=comma, x expr={\thisrow{y} * -3}, y=x] {data/pca_components.csv};

            \ifnum#1=1
                \addplot[only marks, mark=*, color=red] coordinates {
                    (-1.81260902e-16, -1.81260902e-17)
                };
                \draw[red,-stealth,very thick] (axis cs: -1.81260902e-16, -1.81260902e-17) -- (axis cs: -1.81260902e-16 + 0.7, -1.81260902e-17 - 0.1);
                \node[anchor=north, red] at (axis cs: -1.81260902e-16 + 0.35, -1.81260902e-17 - 0.05) {$u$};
            \fi
        \end{axis}
    \end{tikzpicture}
}

\newsavebox{\pls}
\sbox{\pls}{
    \plsplot{0}
}
\newsavebox{\plsvector}
\sbox{\plsvector}{
    \plsplot{1}
}

\begin{frame}{Dimensionality reduction: Partial least squares}
    \begin{tikzpicture}
        \node[] at (-5.25, 3.5) {};
        \node[] at (-5.25, -3.5) {};

        \visible<1>{
            \node[] at (0, 0.5) {
                \usebox{\pcavector}
            };

            \node[] at (0, -3) {
                $v \rightarrow$ direction of maximum variance
            };
        }

        \visible<2>{
            \node[] at (0, 0.5) {
                \usebox{\pcapls}
            };

            \node[] at (0, -3) {
                $u \rightarrow$ direction of greatest covariance between $X$ and $y$
            };
        }
        \visible<3>{
            \node[] at (0, 0.5) {
                \usebox{\pcaz}
            };
        }
        \visible<4>{
            \node[align=left] at (0, 0) {
                $z_{horsepower}* = z_{horsepower} - Z_0$\\
                $z_{weight}* = z_{weight} - Z_0$\\
                $y* = y - Z_0$
            };
        }
        \visible<5>{
            \node[] at (0, 0.5) {
                \usebox{\pls}
            };
        }
        \visible<6>{
            \node[] at (0, 0.5) {
                \usebox{\plsvector}
            };

            \node[] at (0, -3) {
                $u \rightarrow$ direction of greatest covariance between $X*$ and $y*$
            };
        }
        \visible<7>{
            \node[text width=10.5cm] (pca) at (0, 2) {
                \underline{Principal component analysis}: Create orthogonal components that are linear combinations of our variables:\\
                $PC_0=\beta_0x_0+\beta_0x_0+\ldots+\beta_nx_n$,\\
                $PC_1=\gamma_0x_0+\gamma_1x_1+\ldots+\gamma_nx_n$,\\
                $PC_0 \perp PC_1$ (e.g. no linear correlation),\\
                that maximize the variance of $X$
            };
            \node[text width=10.5cm] (ica) at (0, -1) {
                \underline{Partial least squares}: Create orthogonal components that are linear combinations of our variables:\\
                $Z_0=\beta_0x_0+\beta_0x_0+\ldots+\beta_nx_n$,\\
                $Z_1=\gamma_0x_0+\gamma_1x_1+\ldots+\gamma_nx_n$,\\
                $Z_0 \perp PC_1$ (e.g. no linear correlation),\\
                that maximize the covariance between $X$ \textbf{and} $y$
            };
        }
    \end{tikzpicture}
\end{frame}

\begin{frame}{Dimensionality reduction: Summary}
    Dimensionality reduction techniques allow us to reduce the number of variables in our dataset to either aid interpretation, or improve our models through implicit regularization.
    \begin{itemize}
        \item Principal component analysis (PCA): Finds components that are orthogonal and maximize variance
        \item Independent component analysis (ICA): Finds components that are non-Gaussian and statistically independent
        \item Partial least squares (PLS): Finds components that are orthogonal and maximize covariance between $X$ and $y$
    \end{itemize}
\end{frame}
