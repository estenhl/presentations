{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b1074bd-9319-41a0-9679-2a10a011819a",
   "metadata": {},
   "source": [
    "### 0. Prepare the dataset for the subsequent modelling.\n",
    "1. Download the heart disease dataset from https://www.statlearning.com/s/Heart.csv\n",
    "2. Load the dataset and drop all variables except the predictors Age, Sex, ChestPain, RestBP, Chol, and the target variable AHD. Drop all rows containing a NaN value.\n",
    "3. Onehot encode the variable ChestPain. This means that where you before had a single column with one of four values ['typical', 'asymptomatic', 'nonanginal', 'nontypical'], you will now have four binary columns (their names don't matter), akin to 'ChestPain_typical' 'ChestPain_asymptomatic', 'ChestPain_nonanginal', 'ChestPain_nontypical'. A row that before had ChestPain='typical' will now have ChestPain_typical=1 and the other three columns set to 0, ChestPain='asymptomatic' will have ChestPain_asymptomatic=1 and the other three set to 0, etc.\n",
    "4. Binary encode the target variable AHD such that 'No'=0 and 'Yes'=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4549978f-4e28-4edd-bef9-aa114a306ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "predictors = ['Age', 'Sex', 'ChestPain', 'RestBP', 'Chol']\n",
    "target = 'AHD'\n",
    "\n",
    "df = pd.read_csv('https://www.statlearning.com/s/Heart.csv')\n",
    "df = df[predictors + [target]]\n",
    "df = df.dropna()\n",
    "df = pd.get_dummies(df, columns=['ChestPain'])\n",
    "df['AHD'] = df['AHD'].map({'No': 0, 'Yes': 1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb8585-b900-4c09-8298-587c5aadf4d1",
   "metadata": {},
   "source": [
    "### 1. Fit a model using a standard train/validation split through multiple steps.\n",
    "Through the steps you will practice chaining functions, and you will also create the infrastructure necessary for the remaining tasks.\n",
    "\n",
    "1. Write a function \"stratified_split\" that takes three arguments: A dataframe, a number of folds, and a list of variables to stratify by. The function should return a list of dataframes, one for each fold, where the dataframes are stratified by the variables in the list. Test that the function works by splitting the dataset into two folds based on 'AHD', 'Age' and 'RestBP' and print the size of each fold, the counts of 0s and 1s in AHD, and the mean of each of 'Age' and 'RestBP' (all these should be printed individually per fold). Ensure that the function does not modify the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d94da0c-16e8-4b87-a725-527ed58e9703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cc1aa7f-75d4-44b4-8a6c-cd74a2c1cdf0",
   "metadata": {},
   "source": [
    "2. Write a function 'fit_and_predict' that takes 4 arguments: A training set, a validation set, a list of predictors, and a target variable. The function should fit a logistic regression model to the training set using the predictors and target variable, and return the predictions of the model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d6f45e-0509-44f7-9ed5-29e2ddf83aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1745d6e-68fc-4bf4-a394-52ce67f05c5d",
   "metadata": {},
   "source": [
    "3. Write a function 'fit_and_predict_standardized' that takes 5 arguments: A training set, a validation set, a list of predictors, a target variable, and a list of variables to standardize. Using a loop (or a scaler), the function should z-score standardize the given variables in both the training set and the validation set based on the mean and standard deviation in the training set. Then, the function should call the 'fit_and_predict' function and return its result. Ensure that the function does not modify the original dataframes. Test the function using the train and validation set from above (e.g. the two folds from the split), while standardizing the 'Age', 'RestBP' and 'Chol' variables (as mentioned above, the target should be AHD, and you should also include the remaining predictors: 'Sex' and the ChestPain-variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf02185-b4c3-4aed-9706-6e6cd27fcf5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1219322b-7341-48ee-9c9c-a42c01a74358",
   "metadata": {},
   "source": [
    "4. Write a function 'fit_and_compute_auc' that takes 5 arguments: A training set, a validation set, a list of predictors, a target variable, and a list of variables to standardize. The function should call the 'fit_and_predict_standardized' function to retrieve out-of-sample predictions for the validation set. Based on these and the ground truth labels in the validation set, it should compute and return the AUC. Test the function using the train and test set from above, while standardizing the 'Age', 'RestBP' and 'Chol' variables (and including the remaining predictors). Print the AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd5e951-e2bc-4c63-98cf-f04f5b030659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b30ccc72-bdf0-4805-89d4-ed77cbc8fec2",
   "metadata": {},
   "source": [
    "### 2. Perform a cross-validation.\n",
    "Use the 'stratified_split' function to split the dataset into 10 folds, stratified on variables you find reasonable. For each fold, use the 'fit_and_compute_auc' function to compute the AUC of the model on the held-out validation set. Print the mean and standard deviation of the AUCs across the 10 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee22244-c16d-409b-a952-b5f1d01a9a59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91aa1cbe-960a-4695-b0cb-7c0148d28279",
   "metadata": {},
   "source": [
    "### 3. Use the bootstrap to achieve a distribution of out-of-bag AUCs.\n",
    "For 100 iterations, create a bootstrap sample by sampling with replacement from the full dataset until you have a training set equal in size to 80% of the original data. Use the observations not included in the bootstrap sample as the validation set for that iteration.. Fit models and calculate AUCs for each iteration. Print the mean and standard deviation of the AUCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27414f5e-6386-4368-952f-e0c9fb6bbb07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d86b2904-7a0f-4aa7-b946-80021e3743b3",
   "metadata": {},
   "source": [
    "### 4. Theory\n",
    "1. List some benefits of wrapping code in functions rather than copying and pasting it multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9d0894-9f6a-485d-bc99-5701ece00c44",
   "metadata": {},
   "source": [
    "- _Code can be reused in multiple different scenarios, meaning you have to write less code_\n",
    "- _If you discover a bug in the code, that bug only has to be fixed in one place_\n",
    "- _You hide complex implementation details behind a more abstract interface, making it easier to separate concerns_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cabf38-45e9-4768-bf22-5445d1df4916",
   "metadata": {},
   "source": [
    "2.  Explain three classification metrics and their benefits and drawbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efb8b3b-3808-4295-9fe0-9299b7d51772",
   "metadata": {},
   "source": [
    "- _Logloss: Has nice mathematical properties that allows for using it as a loss function when fitting models. Hard to interpret_\n",
    "- _Accuracy: Very intuitive to understand (the proportion of correct predictions). Does not take into account the cost of different misclassifications, __does not handle imbalanced classes__._\n",
    "- _Area under the receiver operating characteristic curve (AUC/AUROC): Does not rely on setting a classification threshold, handles class imbalance. Hard to interpret literally (e.g. an AUC of 0.95 is generally good, but what does it mean more concretely?), can't be used for optimizing models_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa30cbf-ade3-409b-b523-d9d53d17be45",
   "metadata": {},
   "source": [
    "3. Write a couple of sentences comparing the three methods (train/validation, cross-validation, bootstrap) above as approaches to quantify model performance. Which one yielded the best results? Which one would you expect to yield the best results? Can you mention some theoretical benefits and drawbacks with each? Even if you didn't do the optional bootstrap exercise you should reflect on this as an approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0690473-bf22-4b95-b5d9-9ff417600a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train/validation-split AUC: {split_auc:.2f}')\n",
    "print(f'Cross-validation AUC: {np.mean(cv_aucs):.2f}+/-{np.std(cv_aucs):.2f}')\n",
    "print(f'Bootstrap AUC: {np.mean(bootstrap_aucs):.2f}+/-{np.std(bootstrap_aucs):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dcbcc7-4bb2-4a3e-9041-2168f5861ad7",
   "metadata": {},
   "source": [
    "_In our case, the three methods yield results that are statistically equivalent. This is what we would expect, but given the fact that the single train/validation-split has a lot of variance depending on the exact split, this doesn't always happen. As such, one of the two latter is preferable_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d9a690-368f-4c5f-879e-f7778c9db7de",
   "metadata": {},
   "source": [
    "4. Why do we stratify the dataset before splitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19b579-54b2-4e74-83c9-c48845e911cd",
   "metadata": {},
   "source": [
    "_To ensure the different folds of the dataset are similar with respect to certain key variables. If we don't do this, we could arrive at models that are very good in whatever portion they are trained on but very poor in everything else, simply due to the training population not being representative for the rest of the data._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03862d20-8015-476b-aabc-eb25a36a6ed0",
   "metadata": {},
   "source": [
    "5. What other use cases can you think of for the bootstrap method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fab35-4001-47c5-8ba3-510a505bf6c2",
   "metadata": {},
   "source": [
    "_In addition to assessing model performance, the bootstrap can also be used to get an idea of the spread of estimated parameter values (e.g. how important a variable is for a prediction)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a02331-5125-4964-b135-746e9afb148e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
