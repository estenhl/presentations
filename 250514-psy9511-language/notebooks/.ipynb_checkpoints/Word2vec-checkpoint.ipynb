{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ea11bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "NUM_SAMPLES = 1000\n",
    "fileids = np.asarray(movie_reviews.fileids())\n",
    "\n",
    "idx = np.concatenate([\n",
    "    np.arange(NUM_SAMPLES // 2), \n",
    "    np.arange(len(fileids) - NUM_SAMPLES // 2, len(fileids))\n",
    "])\n",
    "\n",
    "print(fileids[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11954da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "reviews = []\n",
    "labels = []\n",
    "\n",
    "for fileid in fileids[idx]:\n",
    "    reviews.append(movie_reviews.raw(fileid))\n",
    "    labels.append(fileid.split('/')[0])\n",
    "    \n",
    "print(reviews[2])\n",
    "print(labels[2])\n",
    "print(Counter(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6992ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from typing import List\n",
    "\n",
    "def tokenize(sentence: List[str], remove_stopwords: bool = False):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        tokens = [token for token in tokens if not token in stopwords.words('english')]\n",
    "    \n",
    "    return tokens\n",
    "    \n",
    "tokens = [tokenize(review, remove_stopwords=True) for review in reviews]\n",
    "print(tokens[:10])\n",
    "y = np.asarray([0 if label == 'neg' else 1 for label in labels])\n",
    "print(y[:10])\n",
    "\n",
    "longest = np.amax([len(sentence) for sentence in tokens])\n",
    "print(f'Longest sentence: {longest}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36986f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load('glove-wiki-gigaword-100')\n",
    "\n",
    "def encode_sentence(sentence: List[str]):\n",
    "    sentence = [word for word in sentence if word in model.key_to_index]\n",
    "    word_vectors = [model[token] for token in sentence]\n",
    "    \n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "X = np.asarray([encode_sentence(sentence) for sentence in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ddc570-1ae9-4ea0-a877-bf0efdb1346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "X[:10, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bba451",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "shuffled_idx = np.random.permutation(np.arange(len(X)))\n",
    "X = X[shuffled_idx]\n",
    "y = y[shuffled_idx]\n",
    "\n",
    "train_idx = int(len(X) * 0.8)\n",
    "\n",
    "train_X = X[:train_idx]\n",
    "train_y = y[:train_idx]\n",
    "test_X = X[train_idx:]\n",
    "test_y = y[train_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1731074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "train_predictions = model.predict_proba(train_X)[:,1]\n",
    "train_auc = roc_auc_score(train_y, train_predictions)\n",
    "print(f'Train AUC: {train_auc:.2f}')\n",
    "\n",
    "test_predictions = model.predict_proba(test_X)[:,1]\n",
    "test_auc = roc_auc_score(test_y, test_predictions)\n",
    "print(f'Test AUC: {test_auc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbdc67a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
