\section{Recap}

\begin{frame}{Recap}
    \begin{itemize}
        \item Lecture 1: Introduction to machine learning
        \begin{itemize}
            \item Supervised vs unsupervised learning
            \item The goal of supervised learning: Find $\hat{y}=f(x)$
        \end{itemize}
        \item Lecture 2: Basics of regression and classification
        \begin{itemize}
            \item Linear regression: $\hat{y}=\beta_0+\sum \beta_jx_j$
            \item Logistic regression: $p(y=1)=\dfrac{e^{\beta_0+\sum \beta_jx_j}}{1+e^{\beta_0+\sum \beta_jx_j}}$
        \end{itemize}
        \item Lecture 3: Variable selection and regularization
        \begin{itemize}
            \item Lasso: $\displaystyle \beta=\min_{\beta}\left(\sum(y_i-\hat{y}_i)^2+\lambda\sum|\beta_j|\right)$
            \item Ridge: $\displaystyle \beta=\min_{\beta}\left(\sum(y_i-\hat{y}_i)^2+\lambda\sum \beta_j^2 \right)$
        \end{itemize}
        \item Lecture 4: Model selection, validation, and testing
        \begin{itemize}
            \item Whenever you make a decision based on results from data, subsequent results on the same data will be inflated
        \end{itemize}
    \end{itemize}
\end{frame}
